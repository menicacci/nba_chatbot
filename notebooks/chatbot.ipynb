{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlMeJaME_PcR",
        "outputId": "c6a8920d-4b51-4063-cb9f-4d13d847351d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ],
      "source": [
        "from scripts import models, sql\n",
        "\n",
        "sql_coder_dir = \"../models/sqlcoder\"\n",
        "sql_model, sql_token = models.get_coder(sql_coder_dir)\n",
        "\n",
        "sql_data_path = \"../dataset/nba_players.sqlite\"\n",
        "db_schema_path = \"../structure/player_data_structure.txt\"\n",
        "\n",
        "with open(db_schema_path, 'r') as file:\n",
        "    db_schema = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azYVA6-9OyNX",
        "outputId": "36516810-3054-456b-dc8d-7eb798a06901"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from Meta-Llama-3-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
            "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
            "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
            "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
            "........................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ],
      "source": [
        "llm_path = \"../models/Meta-Llama-3-8B-Instruct.gguf\"\n",
        "llm = models.get_llm(llm_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYTIS7bBGjZR"
      },
      "outputs": [],
      "source": [
        "sql_prompt = \"\"\"### Task\n",
        "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
        "\n",
        "### Instructions\n",
        "- If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "- Remember that revenue is price multiplied by quantity\n",
        "- Remember that cost is supply_price multiplied by quantity\n",
        "\n",
        "### Database Schema\n",
        "This query will run on a database whose schema is represented in this string:\n",
        "{schema}\n",
        "\n",
        "### Answer\n",
        "Given the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]\n",
        "[SQL]\n",
        "\"\"\"\n",
        "\n",
        "llm_prompt = \"\"\"Q: ### Task\n",
        "Answer this question:  \"{question}\" using the retrieved context as ground truth: \\n\n",
        "CONTEXT: \\n{context}\\n\n",
        "Don't mention the context in your answer, just give a response based on the data provided.\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "def format_llm_prompt(prompt_structure, retrieved_context, question):\n",
        "    return prompt_structure.format(context=retrieved_context, question=question)\n",
        "\n",
        "\n",
        "def format_sql_prompt(prompt_structure, schema, question):\n",
        "    return prompt_structure.format(schema=schema, question=question)\n",
        "\n",
        "\n",
        "def answer(data_path, db_schema, sql_model, sql_token, sql_prompt, llm, llm_prompt, question, print_prompts=False):\n",
        "    sql_prompt = format_sql_prompt(sql_prompt, db_schema, question)\n",
        "    sql_query = models.get_sql_query(sql_model, sql_token, sql_prompt)\n",
        "\n",
        "    retreived_context = sql.get_query_output(data_path, sql_query)\n",
        "    if retreived_context is False or len(retreived_context) == 0:\n",
        "        return \"Unable to generate SQL query\"\n",
        "\n",
        "    llm_prompt = format_llm_prompt(llm_prompt, retreived_context.to_string(), question)\n",
        "\n",
        "    if print_prompts:\n",
        "      print(f\"SQL Query:\\n{sql_query}\")\n",
        "      print(f\"\\nOutput:\\n{retreived_context}\")\n",
        "      print(f\"LLM Prompt:\\n{llm_prompt}\")\n",
        "\n",
        "    return models.get_llm_response(llm, llm_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3H33LQ8_cyB",
        "outputId": "aa1e770d-ac61-4197-d0cb-d5d4a475d130"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =     109.78 ms /    51 runs   (    2.15 ms per token,   464.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   40312.58 ms /   104 tokens (  387.62 ms per token,     2.58 tokens per second)\n",
            "llama_print_timings:        eval time =   33782.76 ms /    50 runs   (  675.66 ms per token,     1.48 tokens per second)\n",
            "llama_print_timings:       total time =   74811.67 ms /   154 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The player who scored the most points in a game during the 2019 season is James Harden with 61 points. This occurred on March 22, 2019, when he played against the San Antonio Spurs (HOU vs. SAS).\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Who is the player that scored the most points in a game during the 2019 season\n",
        "and in which game (when and against who)?\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0FnfsbtVLUS",
        "outputId": "932546e2-9ce1-4229-dfc4-8ea05b9994da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =      61.41 ms /    29 runs   (    2.12 ms per token,   472.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26596.50 ms /    66 tokens (  402.98 ms per token,     2.48 tokens per second)\n",
            "llama_print_timings:        eval time =   16447.38 ms /    28 runs   (  587.41 ms per token,     1.70 tokens per second)\n",
            "llama_print_timings:       total time =   43422.52 ms /    94 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The player who scored the most points during the 2017 season is James Harden. He scored a total of 2490 points overall.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Who is the player that scored the most points during the 2017 season\n",
        "and how many points did he scored overall?\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC6wGJs_Wn3D",
        "outputId": "13f170ac-b2a2-49fe-f866-3a2bd3ad3059"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =     220.79 ms /   106 runs   (    2.08 ms per token,   480.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   43043.74 ms /   108 tokens (  398.55 ms per token,     2.51 tokens per second)\n",
            "llama_print_timings:        eval time =   65267.91 ms /   105 runs   (  621.60 ms per token,     1.61 tokens per second)\n",
            "llama_print_timings:       total time =  109738.80 ms /   213 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 5 players of the 2020 season in terms of points per game are:\n",
            "\n",
            "1. Bradley Beal - 33.187500\n",
            "2. Damian Lillard - 31.578947\n",
            "3. James Harden - 31.184211\n",
            "4. Trae Young - 31.121212\n",
            "5. Russell Westbrook - 29.193548\n",
            "\n",
            "Note: The ranking is based on the average points per game for each player during the 2020 season, as provided in the data.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Show me the top 5 players of the 2020 season in terms of points per game\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNbfhZiVcIh9",
        "outputId": "a1ef38d9-77f0-4de8-bb18-ead096fbe62d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =     213.47 ms /    97 runs   (    2.20 ms per token,   454.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =   45160.85 ms /   117 tokens (  385.99 ms per token,     2.59 tokens per second)\n",
            "llama_print_timings:        eval time =   59943.49 ms /    96 runs   (  624.41 ms per token,     1.60 tokens per second)\n",
            "llama_print_timings:       total time =  106411.32 ms /   213 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 5 players of the 2021 season in terms of average rebounds per game are:\n",
            "\n",
            "1. Clint Capela - 14.0\n",
            "2. Rudy Gobert - 13.96\n",
            "3. Jonas Valanciunas - 12.32\n",
            "4. Domantas Sabonis - 12.02\n",
            "5. Nikola Jokic - 11.73\n",
            "\n",
            "These players had the highest average rebounds per game in the 2021 season.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Show me the top 5 players of the 2021 season in terms of average rebounds per game\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTNRVUS6h8Xg",
        "outputId": "0946ecb5-8aa7-47cb-bc30-be32dfb13c08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =     274.74 ms /   139 runs   (    1.98 ms per token,   505.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =   67183.04 ms /   140 tokens (  479.88 ms per token,     2.08 tokens per second)\n",
            "llama_print_timings:        eval time =   87367.68 ms /   138 runs   (  633.10 ms per token,     1.58 tokens per second)\n",
            "llama_print_timings:       total time =  144458.01 ms /   278 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 10 performances in terms of points scored in a single game are:\n",
            "\n",
            "1. Wilt Chamberlain - 100\n",
            "2. Kobe Bryant - 81\n",
            "3. Wilt Chamberlain - 78\n",
            "4. Wilt Chamberlain - 73\n",
            "5. David Thompson - 73\n",
            "6. Luka Doncic - 73\n",
            "7. Wilt Chamberlain - 72\n",
            "8. Elgin Baylor - 71\n",
            "9. David Robinson - 71\n",
            "10. Wilt Chamberlain - 73\n",
            "\n",
            "Note: There are multiple ties for the top 5 spots, but I've listed all the players who achieved those scores in my response.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Show me the 10 best performances in terms of point scored in a single game\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjT-Yqo1j6ex",
        "outputId": "5d0a56ca-722b-4dd6-a404-49d548f222f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =      43.84 ms /    20 runs   (    2.19 ms per token,   456.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24144.97 ms /    64 tokens (  377.27 ms per token,     2.65 tokens per second)\n",
            "llama_print_timings:        eval time =   11469.43 ms /    19 runs   (  603.65 ms per token,     1.66 tokens per second)\n",
            "llama_print_timings:       total time =   35889.95 ms /    83 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steve Nash has the most assists in a season with a total of 771.0 assists.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Who is the player with the most assists in a season?\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnqKG4_Elpn6",
        "outputId": "ac540a0d-242d-4431-dfef-4def311bd19b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   40312.73 ms\n",
            "llama_print_timings:      sample time =      54.04 ms /    26 runs   (    2.08 ms per token,   481.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   16070.73 ms /    26 runs   (  618.11 ms per token,     1.62 tokens per second)\n",
            "llama_print_timings:       total time =   16412.82 ms /    26 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The player with the most rebounds in a season is Shaquille O'Neal with 26.0 rebounds per game.\n"
          ]
        }
      ],
      "source": [
        "q = \"\"\"\n",
        "Who is the player with the most rebounds in a season?\n",
        "\"\"\"\n",
        "\n",
        "response = answer(sql_data_path,\n",
        "                  db_schema,\n",
        "                  sql_model,\n",
        "                  sql_token,\n",
        "                  sql_prompt,\n",
        "                  llm,\n",
        "                  llm_prompt,\n",
        "                  q)\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
